"""
src/retrieval.py
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Retrieval-–º–æ–¥—É–ª—å: –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ ChromaDB —Ç–∞ –ø–æ—à—É–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö
—á–∞–Ω–∫—ñ–≤ –∑–∞ –∑–∞–ø–∏—Ç–æ–º –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞.

–°—Ç—Ä–∞—Ç–µ–≥—ñ—è –ø–æ—à—É–∫—É (3 —à–∞—Ä–∏):
  1. Semantic search ‚Äî –≤–µ–∫—Ç–æ—Ä–∏ ChromaDB.
  2. Query Expansion ‚Äî —Å–ª–æ–≤–Ω–∏–∫ —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–∏—Ö —Ç–µ—Ä–º—ñ–Ω—ñ–≤ ‚Üí –ø—ñ–¥–∑–∞–ø–∏—Ç–∏.
  3. Keyword Scan ‚Äî –ø–æ–≤–Ω–∏–π –ø–µ—Ä–µ–±—ñ—Ä 111 —á–∞–Ω–∫—ñ–≤ –∑–∞ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏.
     –ì–∞—Ä–∞–Ω—Ç—É—î –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è —Ç–æ—á–Ω–∏—Ö —Ç–∞—Ä–∏—Ñ–Ω–∏—Ö —Ä—è–¥–∫—ñ–≤ ("–ó–Ω—è—Ç—Ç—è 0,9%")
     –Ω–∞–≤—ñ—Ç—å –ø—Ä–∏ semantic mismatch.
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
"""

import logging
import re
from functools import lru_cache
from pathlib import Path

from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è (–º–∞—î –∑–±—ñ–≥–∞—Ç–∏—Å—è –∑ ingest.py)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

PROJECT_ROOT      = Path(__file__).resolve().parent.parent
DEFAULT_DB_DIR    = PROJECT_ROOT / "data" / "chromadb"
EMBEDDING_MODEL   = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
CHROMA_COLLECTION = "finrag_tariffs"

# –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ—ñ–Ω–∞–ª—å–Ω–∏—Ö —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —á–∞–Ω–∫—ñ–≤ —â–æ –ø–æ–≤–µ—Ä—Ç–∞—é—Ç—å—Å—è –Ω–∞ –∑–∞–ø–∏—Ç
DEFAULT_K = 12

log = logging.getLogger(__name__)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –°—Ç–æ–ø-—Å–ª–æ–≤–∞ —Ç–∞ —Å–ª–æ–≤–Ω–∏–∫ Query Expansion
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

_UA_STOP = {
    "—è–∫–∞", "—è–∫–∏–π", "—è–∫–µ", "—è–∫—ñ", "—â–æ", "–¥–µ", "–∫–æ–ª–∏", "—Å–∫—ñ–ª—å–∫–∏", "—á–∏",
    "—è–∫", "–∑–∞", "–Ω–∞", "–ø–æ", "–¥–ª—è", "–≤—ñ–¥", "–¥–æ", "–ø—Ä–∏", "–ø—Ä–æ", "—á–µ—Ä–µ–∑",
    "—É", "–≤", "–∑", "—Ç–∞", "—ñ", "–∞–±–æ", "—Ü–µ", "—î", "–º–µ–Ω—ñ", "–º–µ–Ω–µ", "–º–æ—ó",
    "–±–∞–Ω–∫—É", "–±–∞–Ω–∫", "–±–∞–Ω–∫–∞", "–±–∞–Ω–∫–æ–º", "—É–º–æ–≤–∏", "—É–º–æ–≤", "—É–º–æ–≤–∞—Ö",
    "—ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è", "–¥—ñ–∑–Ω–∞—Ç–∏—Å—è", "–¥—ñ–∑–Ω–∞—Ç–∏—Å—å", "—Ä–æ–∑–∫–∞–∂–∏", "–ø–æ—è—Å–Ω–∏",
}

_TERM_EXPAND: dict[str, list[str]] = {
    "–¥–µ–ø–æ–∑–∏—Ç":        ["–î–µ–ø–æ–∑–∏—Ç –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π", "–¥–µ–ø–æ–∑–∏—Ç–Ω–∏–π –¥–æ–≥–æ–≤—ñ—Ä", "–≤—ñ–¥—Å–æ—Ç–∫–∏ –¥–µ–ø–æ–∑–∏—Ç"],
    "–≤–∫–ª–∞–¥":          ["–î–µ–ø–æ–∑–∏—Ç –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π", "–±–∞–Ω–∫–∞ –Ω–∞–∫–æ–ø–∏—á–µ–Ω–Ω—è"],
    "–Ω–∞–∫–æ–ø–∏—á–µ–Ω–Ω—è":    ["–°–µ—Ä–≤—ñ—Å –Ω–∞–∫–æ–ø–∏—á–µ–Ω–Ω—è –ë–∞–Ω–∫–∞", "–≤—ñ–¥—Å–æ—Ç–∫–∏ –Ω–∞–∫–æ–ø–∏—á–µ–Ω–Ω—è"],
    "–∫—Ä–µ–¥–∏—Ç":         ["–∫—Ä–µ–¥–∏—Ç–Ω–∏–π –ª—ñ–º—ñ—Ç", "–≤—ñ–¥—Å–æ—Ç–∫–æ–≤–∞ —Å—Ç–∞–≤–∫–∞ –∫—Ä–µ–¥–∏—Ç", "–ø—ñ–ª—å–≥–æ–≤–∏–π –ø–µ—Ä—ñ–æ–¥"],
    "–∫–∞—Ä—Ç–∫–∞":         ["–æ–±—Å–ª—É–≥–æ–≤—É–≤–∞–Ω–Ω—è –∫–∞—Ä—Ç–∫–∏", "–æ—Ñ–æ—Ä–º–ª–µ–Ω–Ω—è –∫–∞—Ä—Ç–∫–∏", "—Ç–∏–ø –∫–∞—Ä—Ç–∫–∏ Mastercard"],
    "–∫—Ä–µ–¥–∏—Ç–Ω–∞":       ["–∫—Ä–µ–¥–∏—Ç–Ω–∏–π –ª—ñ–º—ñ—Ç", "–ø—ñ–ª—å–≥–æ–≤–∏–π –ø–µ—Ä—ñ–æ–¥", "–≤—ñ–¥—Å–æ—Ç–∫–∏ –∫–∞—Ä—Ç–∫–∞"],
    "–∫–æ–º—ñ—Å—ñ—è":        ["–∫–æ–º—ñ—Å—ñ—è –∑–Ω—è—Ç—Ç—è", "–∫–æ–º—ñ—Å—ñ—è –ø–µ—Ä–µ–∫–∞–∑", "–±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–æ"],
    "–∑–Ω—è—Ç—Ç—è":         ["–∑–Ω—è—Ç—Ç—è –≤–ª–∞—Å–Ω–∏—Ö –∫–æ—à—Ç—ñ–≤ –∫–∞—Ä—Ç–∫–∞", "–∑–Ω—è—Ç—Ç—è –≥–æ—Ç—ñ–≤–∫–∏ –±–∞–Ω–∫–æ–º–∞—Ç"],
    "–≥–æ—Ç—ñ–≤–∫–∞":        ["–∑–Ω—è—Ç—Ç—è –≤–ª–∞—Å–Ω–∏—Ö –∫–æ—à—Ç—ñ–≤", "–±–∞–Ω–∫–æ–º–∞—Ç –∑–Ω—è—Ç—Ç—è", "–≥–æ—Ç—ñ–≤–∫–∞ –≤—ñ–¥—Å–æ—Ç–æ–∫"],
    "–ø–µ—Ä–µ–∫–∞–∑":        ["–ø–µ—Ä–µ–∫–∞–∑ –º—ñ–∂ –∫–∞—Ä—Ç–∫–∞–º–∏", "–ª—ñ–º—ñ—Ç –ø–µ—Ä–µ–∫–∞–∑—É", "–ø–ª–∞—Ç—ñ–∂ —Ä–µ–∫–≤—ñ–∑–∏—Ç–∏"],
    "–ª—ñ–º—ñ—Ç":          ["–ª—ñ–º—ñ—Ç –ø–µ—Ä–µ–∫–∞–∑—É", "–ª—ñ–º—ñ—Ç –∑–Ω—è—Ç—Ç—è –≥–æ—Ç—ñ–≤–∫–∏", "–∫—Ä–µ–¥–∏—Ç–Ω–∏–π –ª—ñ–º—ñ—Ç"],
    "–æ–±—Å–ª—É–≥–æ–≤—É–≤–∞–Ω–Ω—è": ["–æ–±—Å–ª—É–≥–æ–≤—É–≤–∞–Ω–Ω—è —Ä–∞—Ö—É–Ω–∫—É", "–æ–±—Å–ª—É–≥–æ–≤—É–≤–∞–Ω–Ω—è –∫–∞—Ä—Ç–∫–∏ –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–æ"],
    "–≤—ñ–¥—Å–æ—Ç–æ–∫":       ["–≤—ñ–¥—Å–æ—Ç–∫–æ–≤–∞ —Å—Ç–∞–≤–∫–∞", "—Ä—ñ—á–Ω–∏—Ö", "—Å—Ç–∞–≤–∫–∞ –∫—Ä–µ–¥–∏—Ç"],
    "—Å—Ç–∞–≤–∫–∞":         ["–≤—ñ–¥—Å–æ—Ç–∫–æ–≤–∞ —Å—Ç–∞–≤–∫–∞ –∫—Ä–µ–¥–∏—Ç—É", "—Å—Ç–∞–≤–∫–∞ –¥–µ–ø–æ–∑–∏—Ç—É", "—Ä—ñ—á–Ω–∏—Ö"],
    "—Ñ–æ–ø":            ["–§–û–ü —Ä–∞—Ö—É–Ω–æ–∫", "–ø—ñ–¥–ø—Ä–∏—î–º–µ—Ü—å —Ç–∞—Ä–∏—Ñ–∏", "—Ä–∞—Ö—É–Ω–æ–∫ –§–û–ü"],
}


def _expand_query(query: str) -> list[str]:
    """–ì–µ–Ω–µ—Ä—É—î —Å–ø–∏—Å–æ–∫ –ø—ñ–¥–∑–∞–ø–∏—Ç—ñ–≤ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–ª—é—á–æ–≤–∏—Ö —Ç–µ—Ä–º—ñ–Ω—ñ–≤ —É –∑–∞–ø–∏—Ç—ñ."""
    q_lower = query.lower()
    expansions: list[str] = []
    for trigger, variants in _TERM_EXPAND.items():
        if trigger in q_lower:
            expansions.extend(variants)
    seen = {query.lower()}
    result = []
    for exp in expansions:
        if exp.lower() not in seen:
            seen.add(exp.lower())
            result.append(exp)
    return result[:5]


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Singleton embeddings —Ç–∞ ChromaDB
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@lru_cache(maxsize=1)
def _get_embeddings() -> HuggingFaceEmbeddings:
    """–ü–æ–≤–µ—Ä—Ç–∞—î singleton-–µ–∫–∑–µ–º–ø–ª—è—Ä embedding-–º–æ–¥–µ–ª—ñ."""
    log.info("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è embedding-–º–æ–¥–µ–ª—ñ: %s", EMBEDDING_MODEL)
    return HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )


@lru_cache(maxsize=1)
def _get_vector_store(db_dir: str = str(DEFAULT_DB_DIR)) -> Chroma:
    """–ü–æ–≤–µ—Ä—Ç–∞—î singleton-–ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ ChromaDB."""
    embeddings = _get_embeddings()
    log.info("–ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ ChromaDB: %s", db_dir)
    store = Chroma(
        collection_name=CHROMA_COLLECTION,
        embedding_function=embeddings,
        persist_directory=db_dir,
    )
    count = store._collection.count()
    log.info("ChromaDB –ø—ñ–¥–∫–ª—é—á–µ–Ω–æ. –í–µ–∫—Ç–æ—Ä—ñ–≤ —É –±–∞–∑—ñ: %d", count)
    return store


def _deduplicate(docs: list, limit: int) -> list:
    """–í–∏–¥–∞–ª—è—î –¥—É–±–ª—ñ–∫–∞—Ç–∏ –∑–∞ fingerprint (–ø–µ—Ä—à—ñ 200 —Å–∏–º–≤–æ–ª—ñ–≤)."""
    seen: set[str] = set()
    result: list = []
    for doc in docs:
        fp = doc.page_content[:200].strip()
        if fp not in seen:
            seen.add(fp)
            result.append(doc)
        if len(result) >= limit:
            break
    return result


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Keyword Scan ‚Äî –≥–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–∏–π –ø–æ—à—É–∫ –∑–∞ —Ç–æ—á–Ω–∏–º –ø—ñ–¥—Ä—è–¥–∫–æ–º
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _keyword_scan(query: str, db_dir: str = str(DEFAULT_DB_DIR)) -> list:
    """
    –°–∫–∞–Ω—É—î –í–°–Ü 111 —á–∞–Ω–∫—ñ–≤ —É ChromaDB —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î —Ç—ñ, —â–æ –º—ñ—Å—Ç—è—Ç—å
    –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ —ñ–∑ –∑–∞–ø–∏—Ç—É (—Ç–æ—á–Ω–∏–π –ø–æ—à—É–∫ –ø—ñ–¥—Ä—è–¥–∫–∞).

    –ß–æ–º—É: embedding –º–æ–∂–µ –Ω–µ –∑–≤'—è–∑–∞—Ç–∏ "–∑–Ω—è—Ç—Ç—è –≥–æ—Ç—ñ–≤–∫–∏" ‚Üí "–ó–Ω—è—Ç—Ç—è –≤–ª–∞—Å–Ω–∏—Ö
    –∫–æ—à—Ç—ñ–≤ –∑–∞ –∫–∞—Ä—Ç–∫–æ—é 0,9%" —á–µ—Ä–µ–∑ —Ä—ñ–∑–Ω–∏—Ü—é —É —Ñ–æ—Ä–º—É–ª—é–≤–∞–Ω–Ω—ñ. Keyword scan
    –∑–Ω–∞—Ö–æ–¥–∏—Ç—å —Ü–µ –¥–µ—Ç–µ—Ä–º—ñ–Ω—ñ—Å—Ç–∏—á–Ω–æ –±–µ–∑ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –≤—ñ–¥ –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó —Å—Ö–æ–∂–æ—Å—Ç—ñ.
    """
    store = _get_vector_store(db_dir)
    all_data = store._collection.get(include=["documents", "metadatas"])

    # –ó–Ω–∞—á—É—â—ñ —Å–ª–æ–≤–∞ —ñ–∑ –∑–∞–ø–∏—Ç—É (4+ –ª—ñ—Ç–µ—Ä, –Ω–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞)
    words = re.findall(r"[–∞-—è—ñ—ó—î“ë–ê-–Ø–Ü–á–Ñ“ê]{4,}", query)
    keywords = [w.lower() for w in words if w.lower() not in _UA_STOP]

    if not keywords:
        return []

    matched = []
    for i, text in enumerate(all_data["documents"]):
        t_lower = text.lower()
        if any(kw in t_lower for kw in keywords):
            matched.append(Document(
                page_content=text,
                metadata=all_data["metadatas"][i],
            ))
    return matched


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ü—É–±–ª—ñ—á–Ω–∏–π API
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def retrieve(
    query:   str,
    k:       int  = DEFAULT_K,
    db_dir:  str  = str(DEFAULT_DB_DIR),
    verbose: bool = False,
) -> list:
    """
    –ó–Ω–∞—Ö–æ–¥–∏—Ç—å top-k –Ω–∞–π—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—à–∏—Ö —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —á–∞–Ω–∫—ñ–≤ –¥–æ –∑–∞–ø–∏—Ç—É.

    –ê–ª–≥–æ—Ä–∏—Ç–º (3 —à–∞—Ä–∏):
      1. Semantic search ‚Äî –∑–∞ –ø–æ–≤–Ω–∏–º –∑–∞–ø–∏—Ç–æ–º (k*2 –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤).
      2. Query Expansion ‚Äî –ø—ñ–¥–∑–∞–ø–∏—Ç–∏ –∑–∞ —Å–ª–æ–≤–Ω–∏–∫–æ–º —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–∏—Ö —Ç–µ—Ä–º—ñ–Ω—ñ–≤.
      3. Keyword Scan ‚Äî O(111) –ø–µ—Ä–µ–±—ñ—Ä –∑–∞ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ –∑–∞–ø–∏—Ç—É.
    –í—Å—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –º–µ—Ä–∂–∞—Ç—å—Å—è —Ç–∞ –¥–µ–¥—É–±–ª—ñ–∫—É—é—Ç—å—Å—è ‚Üí top-k —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö.
    """
    store = _get_vector_store(db_dir)
    all_raw: list = []

    # 1. Keyword Scan –ü–ï–†–®–ò–ú ‚Äî –¥–µ—Ç–µ—Ä–º—ñ–Ω–æ–≤–∞–Ω–∏–π —Ç–æ—á–Ω–∏–π –ø–æ—à—É–∫ –º–∞—î –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç
    #    (–≥–∞—Ä–∞–Ω—Ç—É—î —â–æ "–ó–Ω—è—Ç—Ç—è 0,9%" –Ω–µ –≤–∏—Ç—ñ—Å–Ω—è—Ç—å –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ semantic hits)
    kw_docs = _keyword_scan(query, db_dir)
    all_raw.extend(kw_docs)

    # 2. Semantic search ‚Äî –¥–æ–¥–∞—î–º–æ –¥–æ keyword —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    main_docs = store.similarity_search(query, k=k * 2)
    all_raw.extend(main_docs)

    # 3. Query Expansion ‚Äî –ø—ñ–¥–∑–∞–ø–∏—Ç–∏ –∑–∞ —Å–ª–æ–≤–Ω–∏–∫–æ–º —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–∏—Ö —Ç–µ—Ä–º—ñ–Ω—ñ–≤
    sub_queries = _expand_query(query)
    for sub in sub_queries:
        try:
            sub_docs = store.similarity_search(sub, k=4)
            all_raw.extend(sub_docs)
        except Exception as e:
            log.debug("–ü—ñ–¥–∑–∞–ø–∏—Ç '%s' –Ω–µ –≤–¥–∞–≤—Å—è: %s", sub, e)

    # 4. –î–µ–¥—É–±–ª—ñ–∫–∞—Ü—ñ—è + top-k (keyword hits –ø–µ—Ä—à–∏–º–∏ ‚Üí –±–µ—Ä—É—Ç—å—Å—è –≤ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç—ñ)
    docs = _deduplicate(all_raw, limit=k)

    # 5. Verbose debug
    if verbose:
        print(f"\n{'='*60}")
        print(f"üîç DEBUG | query: '{query[:50]}' | k={k}")
        print(f"   semantic={len(main_docs)}, kw_scan={len(kw_docs)}, final={len(docs)}")
        pages = sorted(set(d.metadata.get('page') for d in docs))
        print(f"   —Å—Ç–æ—Ä—ñ–Ω–∫–∏: {pages}")
        for i, doc in enumerate(docs[:3], 1):
            meta = doc.metadata
            print(f"\n   [–ß–∞–Ω–∫ {i}] {meta.get('source')}, —Å—Ç–æ—Ä. {meta.get('page')}")
            print(f"   {doc.page_content[:200]}...")
        print(f"{'='*60}\n")

    log.debug(
        "–ó–∞–ø–∏—Ç: '%s' ‚Üí semantic=%d, kw=%d ‚Üí unique=%d",
        query[:50], len(main_docs), len(kw_docs), len(docs),
    )
    return docs


def extract_sources(docs: list) -> list[dict]:
    """
    –í–∏—Ç—è–≥—É—î —É–Ω—ñ–∫–∞–ª—å–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞ –∑ –º–µ—Ç–∞–¥–∞–Ω–∏—Ö –∑–Ω–∞–π–¥–µ–Ω–∏—Ö —á–∞–Ω–∫—ñ–≤.

    Returns:
        [{"source": "file.pdf", "pages": [2, 3, 6]}, ...]
    """
    sources: dict[str, set] = {}
    for doc in docs:
        src  = doc.metadata.get("source", "–Ω–µ–≤—ñ–¥–æ–º–∏–π –¥–æ–∫—É–º–µ–Ω—Ç")
        page = doc.metadata.get("page", "?")
        sources.setdefault(src, set()).add(page)

    return [
        {"source": src, "pages": sorted(pages, key=lambda p: p if isinstance(p, int) else 0)}
        for src, pages in sources.items()
    ]
